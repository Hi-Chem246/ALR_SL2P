{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biophysical Variable Prediction in Scikit-Learn and Keras using PROSAIL Sentinel 2 Band Simulated Database\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "### Background\n",
    "*Similar background to ALR paper*\n",
    "\n",
    "### Objectives\n",
    "The main objective of this paper is to compare the performance of SL2P(D) against the performance of a purely linear model which performs variable selection on the input bands, as well as the performance of the same linear model with the addition of a neural network which performs regression on the features selected by the linear model. In this case the feature selection will be performed by the least angle regression algorithm (Efron et al., 2002) provided by scikit-learn.\n",
    "\n",
    "Specifically performance will be measured using the root mean squared error of each model as well as the absolute error in the model with respect to the specific true value of the response variables.\n",
    "\n",
    "## Methodology\n",
    "---\n",
    "\n",
    "### Methods\n",
    "Three methods were tested on the global PROSAIL dataset with 41472 samples simulated from various different biomes to estimate LAI, FCOVER, and FAPAR.\n",
    "* SL2P\n",
    "* LARS Regression\n",
    "* LARS Regression + Keras (Tensorflow) Shallow Neural Network\n",
    "\n",
    "The PROSAIL simulation generates Sentinel 2 bands:\n",
    "\n",
    "| Band | Central Wavelength | Description | Resolution | \n",
    "|---|---|---|---|\n",
    "| B3 | 560nm | green | 10m |\n",
    "| B4 | 665nm | red | 10m |\n",
    "| B5 | 705nm | VNIR | 20m res |\n",
    "| B6 | 740nm | VNIR | 20m res |\n",
    "| B7 | 783nm | VNIR | 20m res |\n",
    "| B8A | 865nm | VNIR | 20m res |\n",
    "| B11 | 1610nm | SWIR | 20m res |\n",
    "| B12 | 2190nm | SWIR | 20m res |\n",
    "\n",
    "*SL2P description* + `code`\n",
    "\n",
    "In the script below the methodology used for LARS and the shallow neural network in Keras will be described further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import pandas \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlabData = sio.loadmat(file_name='./data/s2_sl2p_weiss_or_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = pandas.DataFrame(data=matlabData['Input']['Rho_Toc'][0][0])\n",
    "inputAngles = pandas.DataFrame(data=matlabData['Input']['Angles'][0][0])\n",
    "\n",
    "inputDF = pandas.concat([inputDF, inputAngles], axis=1, join='outer', ignore_index=True)\n",
    "inputDF.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3']\n",
    "\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())\n",
    "\n",
    "outputCSV = pandas.concat([inputDF, LAI, FAPAR, FCOVER], axis=1, join='outer')\n",
    "\n",
    "outputCSV['latitude'] = 0\n",
    "outputCSV['longitude'] = 0\n",
    "\n",
    "outputCSV.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', \n",
    "                     'LAI', 'FAPAR', 'FCOVER', 'latitude', 'longitude']\n",
    "outputCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputCSV.to_csv(path_or_buf=r'rawFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability distribution # \n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "print(outputCSV.shape[0])\n",
    "\n",
    "N = numpy.linspace(0,outputCSV.shape[0],outputCSV.shape[0],dtype='int')\n",
    "\n",
    "print(N)\n",
    "\n",
    "pdf = norm.pdf(N, loc=outputCSV.shape[0]/2, scale=outputCSV.shape[0]/6)\n",
    "\n",
    "print(len(pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF_centred = inputDF - inputDF.mean()\n",
    "inputDF_normed = inputDF_centred * inputDF_centred.pow(2).sum().pow(-0.5)\n",
    "\n",
    "LAI_mean = LAI.mean()\n",
    "FAPAR_mean = FAPAR.mean()\n",
    "FCOVER_mean = FCOVER.mean()\n",
    "\n",
    "LAI_centred = LAI.subtract(LAI_mean)\n",
    "FAPAR_centred = FAPAR.subtract(FAPAR_mean)\n",
    "FCOVER_centred = FCOVER.subtract(FCOVER_mean)\n",
    "\n",
    "outputCSV_scaled = pandas.concat([inputDF_normed, LAI_centred, FAPAR_centred, FCOVER_centred], axis=1, join='outer')\n",
    "#outputCSV_scaled['latitude'] = 0\n",
    "#outputCSV_scaled['longitude'] = 0\n",
    "outputCSV_scaled['prob'] = pdf\n",
    "outputCSV_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', \n",
    "                     'LAI', 'FAPAR', 'FCOVER', 'prob']\n",
    "outputCSV_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, valid_data = sklearn.model_selection.train_test_split(outputCSV_scaled, test_size=2000, train_size=10000, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputCSV_scaled.to_csv(path_or_buf=r'scaledFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_training = training_data['LAI']\n",
    "FAPAR_training = training_data['FAPAR']\n",
    "FCOVER_training = training_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_valid = valid_data['LAI']\n",
    "FAPAR_valid = valid_data['FAPAR']\n",
    "FCOVER_valid = valid_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_weights = numpy.array(training_data['prob'])\n",
    "valid_weights = numpy.array(valid_data['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop(['LAI', 'FAPAR', 'FCOVER','prob'], axis=1)\n",
    "valid_data = valid_data.drop(['LAI', 'FAPAR', 'FCOVER','prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "LAImodel = LAImodel.fit(training_data, LAI_training)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "FAPARmodel = FAPARmodel.fit(training_data, FAPAR_training)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "FCOVERmodel = FCOVERmodel.fit(training_data, FCOVER_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_predicted = pandas.Series(LAImodel.predict(valid_data))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(valid_data))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_RMSE = sklearn.metrics.mean_squared_error(LAI_valid, LAI_predicted, squared=False)\n",
    "FAPAR_RMSE = sklearn.metrics.mean_squared_error(FAPAR_valid, FAPAR_predicted, squared=False)\n",
    "FCOVER_RMSE = sklearn.metrics.mean_squared_error(FCOVER_valid, FCOVER_predicted, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LAI Coefficient Path - RMSE: {}'.format(LAI_RMSE))\n",
    "pandas.DataFrame(LAImodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FAPAR Coefficient Path - RMSE: {}'.format(FAPAR_RMSE))\n",
    "pandas.DataFrame(FAPARmodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FCOVER Coefficient Path - RMSE: {}'.format(FCOVER_RMSE))\n",
    "pandas.DataFrame(FCOVERmodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAImodel = LAImodel.fit(training_data, LAI_training)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPARmodel = FAPARmodel.fit(training_data, FAPAR_training)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVERmodel = FCOVERmodel.fit(training_data, FCOVER_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_predicted = pandas.Series(LAImodel.predict(valid_data))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(valid_data))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_features = numpy.nonzero(LAImodel.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPARmodel.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVERmodel.coef_)[0]\n",
    "\n",
    "LAI_features = valid_data.columns[LAI_features]\n",
    "FAPAR_features = valid_data.columns[FAPAR_features]\n",
    "FCOVER_features = valid_data.columns[FCOVER_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = keras.callbacks.History()\n",
    "\n",
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data[LAI_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data[FAPAR_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data[FCOVER_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the sampled inputDF and sampled testing DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_history = LAI_model.fit(x = numpy.array(training_data[LAI_features]), y = numpy.array(LAI_training), \n",
    "                            sample_weight = training_weights,\n",
    "                            epochs = 100,\n",
    "                            validation_data = (numpy.array(valid_data[LAI_features]), numpy.array(LAI_valid), valid_weights) \n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_history = FAPAR_model.fit(x = numpy.array(training_data[FAPAR_features]), y = numpy.array(FAPAR_training),\n",
    "                                sample_weight = training_weights,\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data[FAPAR_features]), numpy.array(FAPAR_valid), valid_weights)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_history = FCOVER_model.fit(x = numpy.array(training_data[FCOVER_features]), y = numpy.array(FCOVER_training),\n",
    "                                  sample_weight = training_weights,\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data[FCOVER_features]), numpy.array(FCOVER_valid), valid_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_predictions = pandas.Series(LAI_model.predict(numpy.array(valid_data[LAI_features])).flatten())\n",
    "FAPAR_predictions = pandas.Series(FAPAR_model.predict(numpy.array(valid_data[FAPAR_features])).flatten())\n",
    "FCOVER_predictions = pandas.Series(FCOVER_model.predict(numpy.array(valid_data[FCOVER_features])).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_skl_LAI = numpy.vstack([LAI_valid, LAI_predicted])\n",
    "xy_tf_LAI = numpy.vstack([LAI_valid, LAI_predictions])\n",
    "\n",
    "xy_skl_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predicted])\n",
    "xy_tf_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predictions])\n",
    "\n",
    "xy_skl_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predicted])\n",
    "xy_tf_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = scipy.stats.gaussian_kde(xy_skl_LAI)(xy_skl_LAI)\n",
    "z_tf_LAI = scipy.stats.gaussian_kde(xy_tf_LAI)(xy_tf_LAI)\n",
    "\n",
    "z_skl_FAPAR = scipy.stats.gaussian_kde(xy_skl_FAPAR)(xy_skl_FAPAR)\n",
    "z_tf_FAPAR = scipy.stats.gaussian_kde(xy_tf_FAPAR)(xy_tf_FAPAR)\n",
    "\n",
    "z_skl_FCOVER = scipy.stats.gaussian_kde(xy_skl_FCOVER)(xy_skl_FCOVER)\n",
    "z_tf_FCOVER = scipy.stats.gaussian_kde(xy_tf_FCOVER)(xy_tf_FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_skl_LAI = z_skl_LAI.argsort()\n",
    "idx_tf_LAI = z_tf_LAI.argsort()\n",
    "\n",
    "idx_skl_FAPAR = z_skl_FAPAR.argsort()\n",
    "idx_tf_FAPAR = z_tf_FAPAR.argsort()\n",
    "\n",
    "idx_skl_FCOVER = z_skl_FCOVER.argsort()\n",
    "idx_tf_FCOVER = z_tf_FCOVER.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skl_LAI = LAI_valid[idx_skl_LAI]\n",
    "x_tf_LAI = LAI_valid[idx_tf_LAI]\n",
    "\n",
    "x_skl_FAPAR = FAPAR_valid[idx_skl_FAPAR]\n",
    "x_tf_FAPAR = FAPAR_valid[idx_tf_FAPAR]\n",
    "\n",
    "x_skl_FCOVER = FCOVER_valid[idx_skl_FCOVER]\n",
    "x_tf_FCOVER = FCOVER_valid[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_skl_LAI = LAI_predicted[idx_skl_LAI]\n",
    "y_tf_LAI = LAI_predictions[idx_tf_LAI]\n",
    "\n",
    "y_skl_FAPAR = FAPAR_predicted[idx_skl_FAPAR]\n",
    "y_tf_FAPAR = FAPAR_predictions[idx_tf_FAPAR]\n",
    "\n",
    "y_skl_FCOVER = FCOVER_predicted[idx_skl_FCOVER]\n",
    "y_tf_FCOVER = FCOVER_predictions[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = z_skl_LAI[idx_skl_LAI]\n",
    "z_tf_LAI = z_tf_LAI[idx_tf_LAI]\n",
    "\n",
    "z_skl_FAPAR = z_skl_FAPAR[idx_skl_FAPAR]\n",
    "z_tf_FAPAR = z_tf_FAPAR[idx_tf_FAPAR]\n",
    "\n",
    "z_skl_FCOVER = z_skl_FCOVER[idx_skl_FCOVER]\n",
    "z_tf_FCOVER = z_tf_FCOVER[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_skl_LAI = sklearn.metrics.mean_squared_error(x_skl_LAI, y_skl_LAI, squared=False)\n",
    "rmse_tf_LAI = sklearn.metrics.mean_squared_error(x_tf_LAI, y_tf_LAI, squared=False)\n",
    "\n",
    "rmse_skl_FAPAR = sklearn.metrics.mean_squared_error(x_skl_FAPAR, y_skl_FAPAR, squared=False)\n",
    "rmse_tf_FAPAR = sklearn.metrics.mean_squared_error(x_tf_FAPAR, y_tf_FAPAR, squared=False)\n",
    "\n",
    "rmse_skl_FCOVER = sklearn.metrics.mean_squared_error(x_skl_FCOVER, y_skl_FCOVER, squared=False)\n",
    "rmse_tf_FCOVER = sklearn.metrics.mean_squared_error(x_tf_FCOVER, y_tf_FCOVER, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(25,15))\n",
    "\n",
    "ax[0,0].scatter(x_skl_LAI, y_skl_LAI, c=z_skl_LAI)\n",
    "ax[0,0].set_title('LASSO LARS LAI - RMSE: {}'.format(rmse_skl_LAI))\n",
    "\n",
    "ax[1,0].scatter(x_tf_LAI, y_tf_LAI, c=z_tf_LAI)\n",
    "ax[1,0].set_title('NNet LAI - RMSE: {}'.format(rmse_tf_LAI))\n",
    "\n",
    "ax[0,1].scatter(x_skl_FAPAR, y_skl_FAPAR, c=z_skl_FAPAR)\n",
    "ax[0,1].set_title('LASSO LARS FAPAR - RMSE: {}'.format(rmse_skl_FAPAR))\n",
    "\n",
    "ax[1,1].scatter(x_tf_FAPAR, y_tf_FAPAR, c=z_tf_FAPAR)\n",
    "ax[1,1].set_title('NNet FAPAR - RMSE: {}'.format(rmse_tf_FAPAR))\n",
    "\n",
    "ax[0,2].scatter(x_skl_FCOVER, y_skl_FCOVER, c=z_skl_FCOVER)\n",
    "ax[0,2].set_title('LASSO LARS FCOVER - RMSE: {}'.format(rmse_skl_FCOVER))\n",
    "\n",
    "ax[1,2].scatter(x_tf_FCOVER, y_tf_FCOVER, c=z_tf_FCOVER)\n",
    "ax[1,2].set_title('NNet FCOVER - RMSE: {}'.format(rmse_tf_FCOVER))\n",
    "\n",
    "#plt.savefig(\"./matplotlib_outputs/random_sampling_w_weights.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_rmse = plt.plot(numpy.sqrt(LAI_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LAI RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_rmse = plt.plot(numpy.sqrt(FAPAR_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FAPAR RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_rmse = plt.plot(numpy.sqrt(FCOVER_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FCOVER RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EE_LARS_Regression(assetName, features, response, maxSamples, n_nonzero):\n",
    "        \n",
    "    inputCSV = ee.FeatureCollection(assetName)\n",
    "    inputCSV = inputCSV.toList(count=maxSamples)\n",
    "    \n",
    "    def extractBands(feature):\n",
    "        feature = ee.Feature(feature)\n",
    "        return feature.toArray(properties=features).toList()\n",
    "\n",
    "    def extractVI(feature):\n",
    "        feature = ee.Feature(feature)\n",
    "        return feature.toArray(properties=[response]).toList()\n",
    "    \n",
    "    inputList = inputCSV.map(extractBands)\n",
    "    outputList = inputCSV.map(extractVI)\n",
    "    \n",
    "    X = ee.Array(inputList)\n",
    "    y = ee.Array(outputList)\n",
    "    \n",
    "    n = X.length().get([0])\n",
    "    m = X.length().get([1])\n",
    "    \n",
    "    \n",
    "    def centre(output):\n",
    "        output = ee.Array(output)\n",
    "        mean = output.reduce(ee.Reducer.mean(), [0]).get([0,0])\n",
    "        return output.subtract(mean)\n",
    "        \n",
    "    def normalize(inputs):\n",
    "        inputs = ee.Array(inputs)\n",
    "        \n",
    "        inputMeans = inputs.reduce(ee.Reducer.mean(), [0])\n",
    "        inputMeans = inputMeans.repeat(0, n)\n",
    "        inputs = inputs.subtract(inputMeans)\n",
    "        inputs = inputs.pow(2).reduce(ee.Reducer.sum(), [0]).pow(-0.5).repeat(0,n).multiply(inputs)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    X = normalize(X)\n",
    "    y = centre(y)\n",
    "    \n",
    "    def LARSregression(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "        prediction = inputs.getArray('prediction')\n",
    "        coeff_arr = inputs.getArray('coeff_arr')\n",
    "    \n",
    "        c = X.matrixTranspose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())\n",
    "\n",
    "        maxLocs = c_abs.gte(C_max.subtract(0.00001))\n",
    "        signs = c.divide(c_abs)\n",
    "\n",
    "        signs_j = maxLocs.multiply(signs).matrixTranspose()\n",
    "        signs_jc = signs_j.abs().subtract(1).multiply(-1)\n",
    "        \n",
    "        A = ee.List(ee.Array([ee.List.sequence(0, m.subtract(1))]).mask(signs_j).toList().get(0))\n",
    "        A_c = ee.List(ee.Array([ee.List.sequence(0, m.subtract(1))]).mask(signs_jc).toList().get(0))\n",
    "\n",
    "        signMatrix_j = signs_j.repeat(0, n)\n",
    "\n",
    "        X_A = X.multiply(signMatrix_j).mask(signs_j)\n",
    "        j = X_A.length().get([1])\n",
    "        \n",
    "        G_A = X_A.matrixTranspose().matrixMultiply(X_A)\n",
    "\n",
    "        V1_A = ee.Array(ee.List.repeat([1], j))\n",
    "\n",
    "        G_Ai = G_A.matrixInverse()\n",
    "\n",
    "        A_A = V1_A.matrixTranspose().matrixMultiply(G_Ai).matrixMultiply(V1_A).get([0,0]).pow(-0.5)\n",
    "\n",
    "        w_A = G_Ai.matrixMultiply(V1_A).multiply(A_A)\n",
    "\n",
    "        u_A = X_A.matrixMultiply(w_A)\n",
    "\n",
    "        a = X.matrixTranspose().matrixMultiply(u_A)\n",
    "\n",
    "        def computeGammaRRay(index_j):\n",
    "            minus_j = C_max.subtract(c.get([index_j, 0])).divide(A_A.subtract(a.get([index_j, 0])))\n",
    "            plus_j = C_max.add(c.get([index_j, 0])).divide(A_A.add(a.get([index_j, 0])))\n",
    "\n",
    "            gammaRRay = ee.Array([minus_j, plus_j]);\n",
    "            gammaRRay = gammaRRay.mask(gammaRRay.gte(0))\n",
    "            gammaRRay = gammaRRay.multiply(-1)\n",
    "\n",
    "            return gammaRRay.get(gammaRRay.argmax())\n",
    "\n",
    "        gammaRRay = ee.Array([A_c.map(computeGammaRRay)])\n",
    "        gamma = gammaRRay.get(gammaRRay.argmax()).multiply(-1)\n",
    "\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "        coefficients = X.matrixSolve(prediction)\n",
    "\n",
    "        def setZero(num):\n",
    "            num = ee.Number(num)\n",
    "            return ee.Algorithms.If(num.abs().lt(0.0000000001), [0], [num])\n",
    "\n",
    "        coefficients = ee.Array(ee.List(coefficients.matrixTranspose().toList().get(0)).map(setZero))\n",
    "\n",
    "        coeff_arr = ee.Array.cat([coeff_arr, coefficients], axis=1)\n",
    "\n",
    "        outputs = ee.Dictionary({'prediction':prediction, 'coeff_arr':coeff_arr})\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    numIterations = ee.List.sequence(1, n_nonzero)\n",
    "    prediction = ee.Array(ee.List.repeat([0], n))\n",
    "    coeff_arr = ee.Array(ee.List.repeat([0], m))\n",
    "    initial = ee.Dictionary({'prediction':prediction, 'coeff_arr':coeff_arr})\n",
    "\n",
    "    finalOutputs = numIterations.iterate(LARSregression, initial)\n",
    "    finalOutputs = ee.Dictionary(finalOutputs)\n",
    "    finalPrediction = finalOutputs.getArray('prediction')\n",
    "\n",
    "    coeff_arr = finalOutputs.getArray('coeff_arr')\n",
    "    coeff_arr = coeff_arr.getInfo()\n",
    "    coeff_arr = numpy.asarray(coeff_arr)\n",
    "    \n",
    "    return coeff_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures', \n",
    "                              ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'], \n",
    "                              'LAI', \n",
    "                              50000, \n",
    "                              10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures',\n",
    "                                ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'],\n",
    "                                'FAPAR', \n",
    "                                50000, \n",
    "                                10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures', \n",
    "                                 ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'],\n",
    "                                 'FCOVER', \n",
    "                                 50000, \n",
    "                                 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef = pandas.DataFrame(LAI_coef)\n",
    "FAPAR_coef = pandas.DataFrame(FAPAR_coef)\n",
    "FCOVER_coef = pandas.DataFrame(FCOVER_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_coef"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
