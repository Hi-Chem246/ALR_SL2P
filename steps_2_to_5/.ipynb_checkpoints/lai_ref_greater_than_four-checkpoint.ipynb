{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries #\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy.matlib\n",
    "import pandas \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from numba import jit\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the precision of the data in the Pandas Dataframes \n",
    "\n",
    "pandas.set_option(\"precision\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any Tensorflow warnings \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MatLab data #\n",
    "\n",
    "matlabData = sio.loadmat(file_name='./data/s2_sl2p_uniform_10_replicates_sobol_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input and output data #\n",
    "\n",
    "bands = pandas.DataFrame(data=matlabData['Input']['Rho_Toc'][0][0])\n",
    "angles = pandas.DataFrame(data=matlabData['Input']['Angles'][0][0])\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>LAI</th>\n",
       "      <th>FAPAR</th>\n",
       "      <th>FCOVER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0033094104</td>\n",
       "      <td>0.0033933009</td>\n",
       "      <td>0.0031395085</td>\n",
       "      <td>-0.0021609234</td>\n",
       "      <td>-0.0034622801</td>\n",
       "      <td>-0.0035805871</td>\n",
       "      <td>-0.0014617342</td>\n",
       "      <td>0.0001377014</td>\n",
       "      <td>-0.0043794833</td>\n",
       "      <td>-0.0035917914</td>\n",
       "      <td>-0.0022237473</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-0.3664171405</td>\n",
       "      <td>-0.3376439765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010750957</td>\n",
       "      <td>0.0048052685</td>\n",
       "      <td>0.0023618409</td>\n",
       "      <td>-0.0003639014</td>\n",
       "      <td>-0.0012843598</td>\n",
       "      <td>-0.0009449439</td>\n",
       "      <td>0.0027310201</td>\n",
       "      <td>0.0044705136</td>\n",
       "      <td>0.0037527329</td>\n",
       "      <td>-0.0035743687</td>\n",
       "      <td>-0.0018591298</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-0.2758468258</td>\n",
       "      <td>-0.2841473161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0022929068</td>\n",
       "      <td>0.0011309476</td>\n",
       "      <td>-0.0028727197</td>\n",
       "      <td>-0.0066173779</td>\n",
       "      <td>-0.0070545659</td>\n",
       "      <td>-0.0072076046</td>\n",
       "      <td>-0.0044201548</td>\n",
       "      <td>-0.0011331458</td>\n",
       "      <td>-0.0005375280</td>\n",
       "      <td>-0.0006417382</td>\n",
       "      <td>0.0025092119</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-0.5649323664</td>\n",
       "      <td>-0.5855197757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.0020559704</td>\n",
       "      <td>-0.0002309697</td>\n",
       "      <td>-0.0017273277</td>\n",
       "      <td>-0.0051560894</td>\n",
       "      <td>-0.0057292826</td>\n",
       "      <td>-0.0056628783</td>\n",
       "      <td>-0.0031721314</td>\n",
       "      <td>-0.0008763904</td>\n",
       "      <td>0.0007902885</td>\n",
       "      <td>0.0012897426</td>\n",
       "      <td>0.0031377984</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-0.4540206403</td>\n",
       "      <td>-0.4447193319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0006766658</td>\n",
       "      <td>0.0103727460</td>\n",
       "      <td>0.0028263351</td>\n",
       "      <td>-0.0031423640</td>\n",
       "      <td>-0.0039174776</td>\n",
       "      <td>-0.0039340306</td>\n",
       "      <td>0.0031744431</td>\n",
       "      <td>0.0049217551</td>\n",
       "      <td>0.0038286154</td>\n",
       "      <td>0.0019817240</td>\n",
       "      <td>-0.0034695628</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-0.5164482028</td>\n",
       "      <td>-0.5613786646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122875</th>\n",
       "      <td>-0.0017966211</td>\n",
       "      <td>-0.0018976856</td>\n",
       "      <td>-0.0020361413</td>\n",
       "      <td>-0.0017789206</td>\n",
       "      <td>-0.0015314958</td>\n",
       "      <td>-0.0014754344</td>\n",
       "      <td>-0.0039703241</td>\n",
       "      <td>-0.0032858939</td>\n",
       "      <td>-0.0043023526</td>\n",
       "      <td>0.0024787674</td>\n",
       "      <td>-0.0035369952</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.1399313717</td>\n",
       "      <td>0.1350865211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122876</th>\n",
       "      <td>0.0002656087</td>\n",
       "      <td>0.0006883065</td>\n",
       "      <td>0.0001012185</td>\n",
       "      <td>-0.0018411151</td>\n",
       "      <td>-0.0019154414</td>\n",
       "      <td>-0.0017897347</td>\n",
       "      <td>-0.0019923222</td>\n",
       "      <td>-0.0003886648</td>\n",
       "      <td>-0.0048677474</td>\n",
       "      <td>0.0028727135</td>\n",
       "      <td>0.0033901822</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0029873394</td>\n",
       "      <td>-0.0145159439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122877</th>\n",
       "      <td>-0.0019563157</td>\n",
       "      <td>-0.0007206394</td>\n",
       "      <td>-0.0017075880</td>\n",
       "      <td>-0.0029944036</td>\n",
       "      <td>-0.0027833101</td>\n",
       "      <td>-0.0029968457</td>\n",
       "      <td>-0.0035358971</td>\n",
       "      <td>-0.0022574491</td>\n",
       "      <td>-0.0016551080</td>\n",
       "      <td>0.0027354754</td>\n",
       "      <td>-0.0037417763</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0129694704</td>\n",
       "      <td>-0.0057634043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122878</th>\n",
       "      <td>-0.0027684148</td>\n",
       "      <td>-0.0017592728</td>\n",
       "      <td>-0.0030936142</td>\n",
       "      <td>0.0008160573</td>\n",
       "      <td>0.0023194837</td>\n",
       "      <td>0.0019416935</td>\n",
       "      <td>-0.0016862533</td>\n",
       "      <td>-0.0029920554</td>\n",
       "      <td>-0.0050871448</td>\n",
       "      <td>0.0020402634</td>\n",
       "      <td>-0.0033231332</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.1755475803</td>\n",
       "      <td>0.1812556883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122879</th>\n",
       "      <td>-0.0015997377</td>\n",
       "      <td>-0.0021927797</td>\n",
       "      <td>-0.0017891924</td>\n",
       "      <td>0.0013378952</td>\n",
       "      <td>0.0031115808</td>\n",
       "      <td>0.0030595773</td>\n",
       "      <td>0.0002360291</td>\n",
       "      <td>-0.0010860300</td>\n",
       "      <td>-0.0054432261</td>\n",
       "      <td>0.0022542853</td>\n",
       "      <td>0.0036524864</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.1706654785</td>\n",
       "      <td>0.1749298508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122880 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  B0            B1            B2            B3            B4  \\\n",
       "0       0.0033094104  0.0033933009  0.0031395085 -0.0021609234 -0.0034622801   \n",
       "1       0.0010750957  0.0048052685  0.0023618409 -0.0003639014 -0.0012843598   \n",
       "2      -0.0022929068  0.0011309476 -0.0028727197 -0.0066173779 -0.0070545659   \n",
       "3      -0.0020559704 -0.0002309697 -0.0017273277 -0.0051560894 -0.0057292826   \n",
       "4      -0.0006766658  0.0103727460  0.0028263351 -0.0031423640 -0.0039174776   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "122875 -0.0017966211 -0.0018976856 -0.0020361413 -0.0017789206 -0.0015314958   \n",
       "122876  0.0002656087  0.0006883065  0.0001012185 -0.0018411151 -0.0019154414   \n",
       "122877 -0.0019563157 -0.0007206394 -0.0017075880 -0.0029944036 -0.0027833101   \n",
       "122878 -0.0027684148 -0.0017592728 -0.0030936142  0.0008160573  0.0023194837   \n",
       "122879 -0.0015997377 -0.0021927797 -0.0017891924  0.0013378952  0.0031115808   \n",
       "\n",
       "                  B5            B6            B7            A1            A2  \\\n",
       "0      -0.0035805871 -0.0014617342  0.0001377014 -0.0043794833 -0.0035917914   \n",
       "1      -0.0009449439  0.0027310201  0.0044705136  0.0037527329 -0.0035743687   \n",
       "2      -0.0072076046 -0.0044201548 -0.0011331458 -0.0005375280 -0.0006417382   \n",
       "3      -0.0056628783 -0.0031721314 -0.0008763904  0.0007902885  0.0012897426   \n",
       "4      -0.0039340306  0.0031744431  0.0049217551  0.0038286154  0.0019817240   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "122875 -0.0014754344 -0.0039703241 -0.0032858939 -0.0043023526  0.0024787674   \n",
       "122876 -0.0017897347 -0.0019923222 -0.0003886648 -0.0048677474  0.0028727135   \n",
       "122877 -0.0029968457 -0.0035358971 -0.0022574491 -0.0016551080  0.0027354754   \n",
       "122878  0.0019416935 -0.0016862533 -0.0029920554 -0.0050871448  0.0020402634   \n",
       "122879  0.0030595773  0.0002360291 -0.0010860300 -0.0054432261  0.0022542853   \n",
       "\n",
       "                  A3  LAI         FAPAR        FCOVER  \n",
       "0      -0.0022237473 -4.5 -0.3664171405 -0.3376439765  \n",
       "1      -0.0018591298 -4.5 -0.2758468258 -0.2841473161  \n",
       "2       0.0025092119 -4.5 -0.5649323664 -0.5855197757  \n",
       "3       0.0031377984 -4.5 -0.4540206403 -0.4447193319  \n",
       "4      -0.0034695628 -4.5 -0.5164482028 -0.5613786646  \n",
       "...              ...  ...           ...           ...  \n",
       "122875 -0.0035369952  4.5  0.1399313717  0.1350865211  \n",
       "122876  0.0033901822  4.5  0.0029873394 -0.0145159439  \n",
       "122877 -0.0037417763  4.5  0.0129694704 -0.0057634043  \n",
       "122878 -0.0033231332  4.5  0.1755475803  0.1812556883  \n",
       "122879  0.0036524864  4.5  0.1706654785  0.1749298508  \n",
       "\n",
       "[122880 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "input_df = pandas.concat([bands, angles], axis=1, join='outer')\n",
    "\n",
    "input_df.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3']\n",
    "\n",
    "input_df_centred = input_df - input_df.mean()\n",
    "\n",
    "input_df_normed = input_df_centred * input_df_centred.pow(2).sum().pow(-0.5)\n",
    "\n",
    "LAI_mean = LAI.mean()\n",
    "FAPAR_mean = FAPAR.mean()\n",
    "FCOVER_mean = FCOVER.mean()\n",
    "\n",
    "LAI_centred = LAI.subtract(LAI_mean)\n",
    "FAPAR_centred = FAPAR.subtract(FAPAR_mean)\n",
    "FCOVER_centred = FCOVER.subtract(FCOVER_mean)\n",
    "\n",
    "cal_data_scaled = pandas.concat([input_df_normed, LAI_centred, FAPAR_centred, FCOVER_centred], axis=1, join='outer')\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset id's #\n",
    "\n",
    "rep = 10\n",
    "\n",
    "subsets = numpy.arange(0, int(cal_data_scaled.shape[0]/10))\n",
    "\n",
    "subset_ids = numpy.matlib.repmat(subsets, 1, rep)\n",
    "\n",
    "cal_data_scaled['subset_id'] = subset_ids[0]\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to create reference database # \n",
    "\n",
    "ref_data = cal_data_scaled[cal_data_scaled['LAI'] > 4].sample(n=100, ignore_index=False)\n",
    "\n",
    "ref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indices to remove from the calibration database #\n",
    "\n",
    "index_list = ref_data.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the reference data so that they start from zero\n",
    "\n",
    "ref_data = ref_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the indices from calibration database that are in the reference database #\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the calibration data so that they start from zero\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates arrays containing the calibration and reference data \n",
    "\n",
    "ref_array = numpy.array(ref_data[['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7','LAI']])\n",
    "\n",
    "cal_array = numpy.array(cal_data_scaled[['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7','LAI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls function from sci-kit learn for calculating the euclidean distance \n",
    "\n",
    "dist = DistanceMetric.get_metric('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the probability for each sample in the calibration data\n",
    "\n",
    "probs = numpy.exp(-numpy.amin(dist.pairwise(cal_array,ref_array),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Probabilities \n",
    "\n",
    "norm_prob = numpy.array(probs)/sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled['prob'] = norm_prob\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the probabilities in a susbset \n",
    "\n",
    "cal_data_scaled[cal_data_scaled['subset_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign max probability in each subset to every member of that subset \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled.groupby('subset_id')['prob'].transform('max')\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled[cal_data_scaled['subset_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renormalize the data \n",
    "\n",
    "re_norm_prob = numpy.array(cal_data_scaled['prob'])/sum(cal_data_scaled['prob'])\n",
    "\n",
    "cal_data_scaled['prob'] = re_norm_prob\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "training_data, valid_data = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=16000, train_size=80000, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_training = training_data['LAI']\n",
    "FAPAR_training = training_data['FAPAR']\n",
    "FCOVER_training = training_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_valid = valid_data['LAI']\n",
    "FAPAR_valid = valid_data['FAPAR']\n",
    "FCOVER_valid = valid_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the probabilites for training and validation \n",
    "\n",
    "training_weights = numpy.array(training_data['prob'])\n",
    "valid_weights = numpy.array(valid_data['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "\n",
    "training_data = training_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)\n",
    "valid_data = valid_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model for LAI, FAPAR, and FCOVER using LARs regression \n",
    "\n",
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAImodel = LAImodel.fit(training_data, LAI_training)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPARmodel = FAPARmodel.fit(training_data, FAPAR_training)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVERmodel = FCOVERmodel.fit(training_data, FCOVER_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the LARS models\n",
    "\n",
    "LAI_predicted = pandas.Series(LAImodel.predict(valid_data))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(valid_data))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the features from the LARS Model\n",
    "\n",
    "LAI_features = numpy.nonzero(LAImodel.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPARmodel.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVERmodel.coef_)[0]\n",
    "\n",
    "LAI_features = valid_data.columns[LAI_features]\n",
    "FAPAR_features = valid_data.columns[FAPAR_features]\n",
    "FCOVER_features = valid_data.columns[FCOVER_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history = LAI_model.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            sample_weight = training_weights,\n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid), valid_weights) \n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FAPAR\n",
    "\n",
    "FAPAR_history = FAPAR_model.fit(x = numpy.array(training_data), y = numpy.array(FAPAR_training),\n",
    "                                sample_weight = training_weights,\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data), numpy.array(FAPAR_valid), valid_weights)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FCOVER\n",
    "\n",
    "FCOVER_history = FCOVER_model.fit(x = numpy.array(training_data), y = numpy.array(FCOVER_training),\n",
    "                                  sample_weight = training_weights,\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data), numpy.array(FCOVER_valid), valid_weights)\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all plots for the RMSE of the NN as training was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_rmse = plt.plot(numpy.sqrt(LAI_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LAI RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_rmse = plt.plot(numpy.sqrt(FAPAR_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FAPAR RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_rmse = plt.plot(numpy.sqrt(FCOVER_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FCOVER RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the \n",
    "\n",
    "LAI_predictions = pandas.Series(LAI_model.predict(numpy.array(valid_data)).flatten())\n",
    "FAPAR_predictions = pandas.Series(FAPAR_model.predict(numpy.array(valid_data)).flatten())\n",
    "FCOVER_predictions = pandas.Series(FCOVER_model.predict(numpy.array(valid_data)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few cells are used to create density plots for the predictions against the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_skl_LAI = numpy.vstack([LAI_valid, LAI_predicted])\n",
    "xy_tf_LAI = numpy.vstack([LAI_valid, LAI_predictions])\n",
    "\n",
    "xy_skl_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predicted])\n",
    "xy_tf_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predictions])\n",
    "\n",
    "xy_skl_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predicted])\n",
    "xy_tf_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = scipy.stats.gaussian_kde(xy_skl_LAI)(xy_skl_LAI)\n",
    "z_tf_LAI = scipy.stats.gaussian_kde(xy_tf_LAI)(xy_tf_LAI)\n",
    "\n",
    "z_skl_FAPAR = scipy.stats.gaussian_kde(xy_skl_FAPAR)(xy_skl_FAPAR)\n",
    "z_tf_FAPAR = scipy.stats.gaussian_kde(xy_tf_FAPAR)(xy_tf_FAPAR)\n",
    "\n",
    "z_skl_FCOVER = scipy.stats.gaussian_kde(xy_skl_FCOVER)(xy_skl_FCOVER)\n",
    "z_tf_FCOVER = scipy.stats.gaussian_kde(xy_tf_FCOVER)(xy_tf_FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_skl_LAI = z_skl_LAI.argsort()\n",
    "idx_tf_LAI = z_tf_LAI.argsort()\n",
    "\n",
    "idx_skl_FAPAR = z_skl_FAPAR.argsort()\n",
    "idx_tf_FAPAR = z_tf_FAPAR.argsort()\n",
    "\n",
    "idx_skl_FCOVER = z_skl_FCOVER.argsort()\n",
    "idx_tf_FCOVER = z_tf_FCOVER.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skl_LAI = LAI_valid[idx_skl_LAI]\n",
    "x_tf_LAI = LAI_valid[idx_tf_LAI]\n",
    "\n",
    "x_skl_FAPAR = FAPAR_valid[idx_skl_FAPAR]\n",
    "x_tf_FAPAR = FAPAR_valid[idx_tf_FAPAR]\n",
    "\n",
    "x_skl_FCOVER = FCOVER_valid[idx_skl_FCOVER]\n",
    "x_tf_FCOVER = FCOVER_valid[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_skl_LAI = LAI_predicted[idx_skl_LAI]\n",
    "y_tf_LAI = LAI_predictions[idx_tf_LAI]\n",
    "\n",
    "y_skl_FAPAR = FAPAR_predicted[idx_skl_FAPAR]\n",
    "y_tf_FAPAR = FAPAR_predictions[idx_tf_FAPAR]\n",
    "\n",
    "y_skl_FCOVER = FCOVER_predicted[idx_skl_FCOVER]\n",
    "y_tf_FCOVER = FCOVER_predictions[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = z_skl_LAI[idx_skl_LAI]\n",
    "z_tf_LAI = z_tf_LAI[idx_tf_LAI]\n",
    "\n",
    "z_skl_FAPAR = z_skl_FAPAR[idx_skl_FAPAR]\n",
    "z_tf_FAPAR = z_tf_FAPAR[idx_tf_FAPAR]\n",
    "\n",
    "z_skl_FCOVER = z_skl_FCOVER[idx_skl_FCOVER]\n",
    "z_tf_FCOVER = z_tf_FCOVER[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_skl_LAI = sklearn.metrics.mean_squared_error(x_skl_LAI, y_skl_LAI, squared=False)\n",
    "rmse_tf_LAI = sklearn.metrics.mean_squared_error(x_tf_LAI, y_tf_LAI, squared=False)\n",
    "\n",
    "rmse_skl_FAPAR = sklearn.metrics.mean_squared_error(x_skl_FAPAR, y_skl_FAPAR, squared=False)\n",
    "rmse_tf_FAPAR = sklearn.metrics.mean_squared_error(x_tf_FAPAR, y_tf_FAPAR, squared=False)\n",
    "\n",
    "rmse_skl_FCOVER = sklearn.metrics.mean_squared_error(x_skl_FCOVER, y_skl_FCOVER, squared=False)\n",
    "rmse_tf_FCOVER = sklearn.metrics.mean_squared_error(x_tf_FCOVER, y_tf_FCOVER, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(25,15))\n",
    "\n",
    "ax[0,0].scatter(x_skl_LAI, y_skl_LAI, c=z_skl_LAI)\n",
    "ax[0,0].set_title('LASSO LARS LAI - RMSE: {}'.format(rmse_skl_LAI))\n",
    "\n",
    "ax[1,0].scatter(x_tf_LAI, y_tf_LAI, c=z_tf_LAI)\n",
    "ax[1,0].set_title('NNet LAI - RMSE: {}'.format(rmse_tf_LAI))\n",
    "\n",
    "ax[0,1].scatter(x_skl_FAPAR, y_skl_FAPAR, c=z_skl_FAPAR)\n",
    "ax[0,1].set_title('LASSO LARS FAPAR - RMSE: {}'.format(rmse_skl_FAPAR))\n",
    "\n",
    "ax[1,1].scatter(x_tf_FAPAR, y_tf_FAPAR, c=z_tf_FAPAR)\n",
    "ax[1,1].set_title('NNet FAPAR - RMSE: {}'.format(rmse_tf_FAPAR))\n",
    "\n",
    "ax[0,2].scatter(x_skl_FCOVER, y_skl_FCOVER, c=z_skl_FCOVER)\n",
    "ax[0,2].set_title('LASSO LARS FCOVER - RMSE: {}'.format(rmse_skl_FCOVER))\n",
    "\n",
    "ax[1,2].scatter(x_tf_FCOVER, y_tf_FCOVER, c=z_tf_FCOVER)\n",
    "ax[1,2].set_title('NNet FCOVER - RMSE: {}'.format(rmse_tf_FCOVER))\n",
    "\n",
    "#plt.savefig(\"./matplotlib_outputs/uniform_replicates_w_weights_50000_samples_all_features.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These models will be done without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history_no_weights = LAI_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid)) \n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FAPAR\n",
    "\n",
    "FAPAR_history_no_weights = FAPAR_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(FAPAR_training),\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data), numpy.array(FAPAR_valid))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FCOVER\n",
    "\n",
    "FCOVER_history_no_weights = FCOVER_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(FCOVER_training),\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data), numpy.array(FCOVER_valid))\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the \n",
    "\n",
    "LAI_predictions_no_weights = pandas.Series(LAI_model_no_weights.predict(numpy.array(valid_data)).flatten())\n",
    "FAPAR_predictions_no_weights = pandas.Series(FAPAR_model_no_weights.predict(numpy.array(valid_data)).flatten())\n",
    "FCOVER_predictions_no_weights = pandas.Series(FCOVER_model_no_weights.predict(numpy.array(valid_data)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few cells are used to create density plots for the predictions against the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy_skl_LAI = numpy.vstack([LAI_valid, LAI_predicted])\n",
    "xy_tf_LAI_no_weights = numpy.vstack([LAI_valid, LAI_predictions_no_weights])\n",
    "\n",
    "xy_skl_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predicted])\n",
    "xy_tf_FAPAR_no_weights = numpy.vstack([FAPAR_valid, FAPAR_predictions_no_weights])\n",
    "\n",
    "xy_skl_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predicted])\n",
    "xy_tf_FCOVER_no_weights = numpy.vstack([FCOVER_valid, FCOVER_predictions_no_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = scipy.stats.gaussian_kde(xy_skl_LAI_no_weights)(xy_skl_LAI_no_weights)\n",
    "z_tf_LAI_no_weights = scipy.stats.gaussian_kde(xy_tf_LAI_no_weights)(xy_tf_LAI_no_weights)\n",
    "\n",
    "z_skl_FAPAR = scipy.stats.gaussian_kde(xy_skl_FAPAR_no_weights)(xy_skl_FAPAR_no_weights)\n",
    "z_tf_FAPAR_no_weights = scipy.stats.gaussian_kde(xy_tf_FAPAR_no_weights)(xy_tf_FAPAR_no_weights)\n",
    "\n",
    "z_skl_FCOVER = scipy.stats.gaussian_kde(xy_skl_FCOVER_no_weights)(xy_skl_FCOVER_no_weights)\n",
    "z_tf_FCOVER_no_weights = scipy.stats.gaussian_kde(xy_tf_FCOVER_no_weights)(xy_tf_FCOVER_no_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_skl_LAI = z_skl_LAI_no_weights.argsort()\n",
    "idx_tf_LAI_no_weights = z_tf_LAI_no_weights.argsort()\n",
    "\n",
    "idx_skl_FAPAR = z_skl_FAPAR_no_weights.argsort()\n",
    "idx_tf_FAPAR_no_weights = z_tf_FAPAR_no_weights.argsort()\n",
    "\n",
    "idx_skl_FCOVER = z_skl_FCOVER_no_weights.argsort()\n",
    "idx_tf_FCOVER_no_weights = z_tf_FCOVER_no_weights.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skl_LAI = LAI_valid[idx_skl_LAI_no_weights]\n",
    "x_tf_LAI_no_weights = LAI_valid[idx_tf_LAI_no_weights]\n",
    "\n",
    "x_skl_FAPAR = FAPAR_valid[idx_skl_FAPAR_no_weights]\n",
    "x_tf_FAPAR_no_weights = FAPAR_valid[idx_tf_FAPAR_no_weights]\n",
    "\n",
    "x_skl_FCOVER = FCOVER_valid[idx_skl_FCOVER_no_weights]\n",
    "x_tf_FCOVER_no_weights = FCOVER_valid[idx_tf_FCOVER_no_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_skl_LAI = LAI_predicteds[idx_skl_LAI]\n",
    "y_tf_LAI_no_weights = LAI_predictions_no_weights[idx_tf_LAI_no_weights]\n",
    "\n",
    "y_skl_FAPAR = FAPAR_predicted[idx_skl_FAPAR]\n",
    "y_tf_FAPAR_no_weights = FAPAR_predictions_no_weights[idx_tf_FAPAR_no_weights]\n",
    "\n",
    "y_skl_FCOVER = FCOVER_predicted[idx_skl_FCOVER]\n",
    "y_tf_FCOVER_no_weights = FCOVER_predictions_no_weights[idx_tf_FCOVER_no_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = z_skl_LAI[idx_skl_LAI]\n",
    "z_tf_LAI_no_weights = z_tf_LAI_no_weights[idx_tf_LAI_no_weights]\n",
    "\n",
    "z_skl_FAPAR = z_skl_FAPAR[idx_skl_FAPAR]\n",
    "z_skl_FAPAR_no_weights = z_skl_FAPAR_no_weights[idx_skl_FAPAR_no_weights]\n",
    "\n",
    "z_skl_FCOVER = z_skl_FCOVER[idx_skl_FCOVER]\n",
    "z_tf_FCOVER_no_weights = z_tf_FCOVER[idx_tf_FCOVER_no_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_skl_LAI = sklearn.metrics.mean_squared_error(x_skl_LAI, y_skl_LAI, squared=False)\n",
    "rmse_tf_LAI_no_weights = sklearn.metrics.mean_squared_error(x_tf_LAI_no_weight, y_tf_LAI_no_weight, squared=False)\n",
    "\n",
    "rmse_skl_FAPAR = sklearn.metrics.mean_squared_error(x_skl_FAPAR, y_skl_FAPAR, squared=False)\n",
    "rmse_tf_FAPAR_no_weight = sklearn.metrics.mean_squared_error(x_tf_FAPAR_no_weight, y_tf_FAPAR_no_weight, squared=False)\n",
    "\n",
    "rmse_skl_FCOVER = sklearn.metrics.mean_squared_error(x_skl_FCOVER, y_skl_FCOVER, squared=False)\n",
    "rmse_tf_FCOVER_no_weight = sklearn.metrics.mean_squared_error(x_tf_FCOVER_no_weight, y_tf_FCOVER_no_weight, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(25,15))\n",
    "\n",
    "ax[0,0].scatter(x_skl_LAI, y_skl_LAI, c=z_skl_LAI)\n",
    "ax[0,0].set_title('LASSO LARS LAI - RMSE: {}'.format(rmse_skl_LAI))\n",
    "\n",
    "ax[1,0].scatter(x_tf_LAI_no_weight, y_tf_LAI_no_weight, c=z_tf_LAI_no_weight)\n",
    "ax[1,0].set_title('NNet LAI - RMSE: {}'.format(rmse_tf_LAI_no_weight))\n",
    "\n",
    "ax[0,1].scatter(x_skl_FAPAR, y_skl_FAPAR, c=z_skl_FAPAR)\n",
    "ax[0,1].set_title('LASSO LARS FAPAR - RMSE: {}'.format(rmse_skl_FAPAR))\n",
    "\n",
    "ax[1,1].scatter(x_tf_FAPAR_no_weight, y_tf_FAPAR_no_weight, c=z_tf_FAPAR_no_weight)\n",
    "ax[1,1].set_title('NNet FAPAR - RMSE: {}'.format(rmse_tf_FAPAR_no_weight))\n",
    "\n",
    "ax[0,2].scatter(x_skl_FCOVER, y_skl_FCOVER, c=z_skl_FCOVER)\n",
    "ax[0,2].set_title('LASSO LARS FCOVER - RMSE: {}'.format(rmse_skl_FCOVER))\n",
    "\n",
    "ax[1,2].scatter(x_tf_FCOVER_no_weight, y_tf_FCOVER_no_weight, c=z_tf_FCOVER_no_weight)\n",
    "ax[1,2].set_title('NNet FCOVER - RMSE: {}'.format(rmse_tf_FCOVER_no_weight))\n",
    "\n",
    "#plt.savefig(\"./matplotlib_outputs/.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all plots for the RMSE of the NN as training was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_rmse = plt.plot(numpy.sqrt(LAI_history_no_weight.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LAI RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_rmse = plt.plot(numpy.sqrt(FAPAR_history_no_weight.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FAPAR RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_rmse = plt.plot(numpy.sqrt(FCOVER_history_no_weight.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FCOVER RMSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
