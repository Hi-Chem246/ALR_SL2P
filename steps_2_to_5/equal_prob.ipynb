{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries #\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy.matlib\n",
    "import pandas \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from numba import jit\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the precision of the data in the Pandas Dataframes \n",
    "\n",
    "pandas.set_option(\"precision\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any Tensorflow warnings \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MatLab data #\n",
    "\n",
    "matlabData = sio.loadmat(file_name='./data/s2_sl2p_uniform_10_replicates_sobol_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input and output data #\n",
    "\n",
    "bands = pandas.DataFrame(data=matlabData['Input']['Rho_Toc'][0][0])\n",
    "angles = pandas.DataFrame(data=matlabData['Input']['Angles'][0][0])\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "input_df = pandas.concat([bands, angles], axis=1, join='outer')\n",
    "\n",
    "input_df.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3']\n",
    "\n",
    "input_df_centred = input_df - input_df.mean()\n",
    "\n",
    "input_df_normed = input_df_centred * input_df_centred.pow(2).sum().pow(-0.5)\n",
    "\n",
    "LAI_mean = LAI.mean()\n",
    "FAPAR_mean = FAPAR.mean()\n",
    "FCOVER_mean = FCOVER.mean()\n",
    "\n",
    "LAI_centred = LAI.subtract(LAI_mean)\n",
    "FAPAR_centred = FAPAR.subtract(FAPAR_mean)\n",
    "FCOVER_centred = FCOVER.subtract(FCOVER_mean)\n",
    "\n",
    "cal_data_scaled = pandas.concat([input_df_normed, LAI_centred, FAPAR_centred, FCOVER_centred], axis=1, join='outer')\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset id's #\n",
    "\n",
    "rep = 10\n",
    "\n",
    "subsets = numpy.arange(0, int(cal_data_scaled.shape[0]/10))\n",
    "\n",
    "subset_ids = numpy.matlib.repmat(subsets, 1, rep)\n",
    "\n",
    "cal_data_scaled['subset_id'] = subset_ids[0]\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to create reference database # \n",
    "\n",
    "ref_data = cal_data_scaled.sample(n=100, ignore_index=False)\n",
    "\n",
    "ref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indices to remove from the calibration database #\n",
    "\n",
    "index_list = ref_data.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the reference data so that they start from zero\n",
    "\n",
    "ref_data = ref_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the indices from calibration database that are in the reference database #\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the calibration data so that they start from zero\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates arrays containing the calibration and reference data \n",
    "\n",
    "ref_array = numpy.array(ref_data[['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7','LAI']])\n",
    "\n",
    "cal_array = numpy.array(cal_data_scaled[['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7','LAI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls function from sci-kit learn for calculating the euclidean distance \n",
    "\n",
    "dist = DistanceMetric.get_metric('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the probability for each sample in the calibration data\n",
    "\n",
    "probs = numpy.exp(-numpy.amin(dist.pairwise(cal_array,ref_array),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Probabilities \n",
    "\n",
    "norm_prob = numpy.array(probs)/sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled['prob'] = norm_prob\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the probabilities in a susbset \n",
    "\n",
    "cal_data_scaled[cal_data_scaled['subset_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign max probability in each subset to every member of that subset \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled.groupby('subset_id')['prob'].transform('max')\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled[cal_data_scaled['subset_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renormalize the data \n",
    "\n",
    "re_norm_prob = numpy.array(cal_data_scaled['prob'])/sum(cal_data_scaled['prob'])\n",
    "\n",
    "cal_data_scaled['prob'] = re_norm_prob\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "training_data, valid_data = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=16000, train_size=80000, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_training = training_data['LAI']\n",
    "FAPAR_training = training_data['FAPAR']\n",
    "FCOVER_training = training_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_valid = valid_data['LAI']\n",
    "FAPAR_valid = valid_data['FAPAR']\n",
    "FCOVER_valid = valid_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the probabilites for training and validation \n",
    "\n",
    "training_weights = numpy.array(training_data['prob'])\n",
    "valid_weights = numpy.array(valid_data['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "\n",
    "training_data = training_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)\n",
    "valid_data = valid_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model for LAI, FAPAR, and FCOVER using LARs regression \n",
    "\n",
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAImodel = LAImodel.fit(training_data, LAI_training)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPARmodel = FAPARmodel.fit(training_data, FAPAR_training)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVERmodel = FCOVERmodel.fit(training_data, FCOVER_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the LARS models\n",
    "\n",
    "LAI_predicted = pandas.Series(LAImodel.predict(valid_data))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(valid_data))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the features from the LARS Model\n",
    "\n",
    "LAI_features = numpy.nonzero(LAImodel.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPARmodel.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVERmodel.coef_)[0]\n",
    "\n",
    "LAI_features = valid_data.columns[LAI_features]\n",
    "FAPAR_features = valid_data.columns[FAPAR_features]\n",
    "FCOVER_features = valid_data.columns[FCOVER_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history = LAI_model.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid)) \n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FAPAR\n",
    "\n",
    "FAPAR_history = FAPAR_model.fit(x = numpy.array(training_data), y = numpy.array(FAPAR_training),\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data), numpy.array(FAPAR_valid))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FCOVER\n",
    "\n",
    "FCOVER_history = FCOVER_model.fit(x = numpy.array(training_data), y = numpy.array(FCOVER_training),\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data), numpy.array(FCOVER_valid))\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the \n",
    "\n",
    "LAI_predictions = pandas.Series(LAI_model.predict(numpy.array(valid_data)).flatten())\n",
    "FAPAR_predictions = pandas.Series(FAPAR_model.predict(numpy.array(valid_data)).flatten())\n",
    "FCOVER_predictions = pandas.Series(FCOVER_model.predict(numpy.array(valid_data)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few cells are used to create density plots for the predictions against the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_skl_LAI = numpy.vstack([LAI_valid, LAI_predicted])\n",
    "xy_tf_LAI = numpy.vstack([LAI_valid, LAI_predictions])\n",
    "\n",
    "xy_skl_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predicted])\n",
    "xy_tf_FAPAR = numpy.vstack([FAPAR_valid, FAPAR_predictions])\n",
    "\n",
    "xy_skl_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predicted])\n",
    "xy_tf_FCOVER = numpy.vstack([FCOVER_valid, FCOVER_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = scipy.stats.gaussian_kde(xy_skl_LAI)(xy_skl_LAI)\n",
    "z_tf_LAI = scipy.stats.gaussian_kde(xy_tf_LAI)(xy_tf_LAI)\n",
    "\n",
    "z_skl_FAPAR = scipy.stats.gaussian_kde(xy_skl_FAPAR)(xy_skl_FAPAR)\n",
    "z_tf_FAPAR = scipy.stats.gaussian_kde(xy_tf_FAPAR)(xy_tf_FAPAR)\n",
    "\n",
    "z_skl_FCOVER = scipy.stats.gaussian_kde(xy_skl_FCOVER)(xy_skl_FCOVER)\n",
    "z_tf_FCOVER = scipy.stats.gaussian_kde(xy_tf_FCOVER)(xy_tf_FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_skl_LAI = z_skl_LAI.argsort()\n",
    "idx_tf_LAI = z_tf_LAI.argsort()\n",
    "\n",
    "idx_skl_FAPAR = z_skl_FAPAR.argsort()\n",
    "idx_tf_FAPAR = z_tf_FAPAR.argsort()\n",
    "\n",
    "idx_skl_FCOVER = z_skl_FCOVER.argsort()\n",
    "idx_tf_FCOVER = z_tf_FCOVER.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skl_LAI = LAI_valid[idx_skl_LAI]\n",
    "x_tf_LAI = LAI_valid[idx_tf_LAI]\n",
    "\n",
    "x_skl_FAPAR = FAPAR_valid[idx_skl_FAPAR]\n",
    "x_tf_FAPAR = FAPAR_valid[idx_tf_FAPAR]\n",
    "\n",
    "x_skl_FCOVER = FCOVER_valid[idx_skl_FCOVER]\n",
    "x_tf_FCOVER = FCOVER_valid[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_skl_LAI = LAI_predicted[idx_skl_LAI]\n",
    "y_tf_LAI = LAI_predictions[idx_tf_LAI]\n",
    "\n",
    "y_skl_FAPAR = FAPAR_predicted[idx_skl_FAPAR]\n",
    "y_tf_FAPAR = FAPAR_predictions[idx_tf_FAPAR]\n",
    "\n",
    "y_skl_FCOVER = FCOVER_predicted[idx_skl_FCOVER]\n",
    "y_tf_FCOVER = FCOVER_predictions[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = z_skl_LAI[idx_skl_LAI]\n",
    "z_tf_LAI = z_tf_LAI[idx_tf_LAI]\n",
    "\n",
    "z_skl_FAPAR = z_skl_FAPAR[idx_skl_FAPAR]\n",
    "z_tf_FAPAR = z_tf_FAPAR[idx_tf_FAPAR]\n",
    "\n",
    "z_skl_FCOVER = z_skl_FCOVER[idx_skl_FCOVER]\n",
    "z_tf_FCOVER = z_tf_FCOVER[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_skl_LAI = sklearn.metrics.mean_squared_error(x_skl_LAI, y_skl_LAI, squared=False)\n",
    "rmse_tf_LAI = sklearn.metrics.mean_squared_error(x_tf_LAI, y_tf_LAI, squared=False)\n",
    "\n",
    "rmse_skl_FAPAR = sklearn.metrics.mean_squared_error(x_skl_FAPAR, y_skl_FAPAR, squared=False)\n",
    "rmse_tf_FAPAR = sklearn.metrics.mean_squared_error(x_tf_FAPAR, y_tf_FAPAR, squared=False)\n",
    "\n",
    "rmse_skl_FCOVER = sklearn.metrics.mean_squared_error(x_skl_FCOVER, y_skl_FCOVER, squared=False)\n",
    "rmse_tf_FCOVER = sklearn.metrics.mean_squared_error(x_tf_FCOVER, y_tf_FCOVER, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(25,15))\n",
    "\n",
    "ax[0,0].scatter(x_skl_LAI, y_skl_LAI, c=z_skl_LAI)\n",
    "ax[0,0].set_title('LASSO LARS LAI - RMSE: {}'.format(rmse_skl_LAI))\n",
    "\n",
    "ax[1,0].scatter(x_tf_LAI, y_tf_LAI, c=z_tf_LAI)\n",
    "ax[1,0].set_title('NNet LAI - RMSE: {}'.format(rmse_tf_LAI))\n",
    "\n",
    "ax[0,1].scatter(x_skl_FAPAR, y_skl_FAPAR, c=z_skl_FAPAR)\n",
    "ax[0,1].set_title('LASSO LARS FAPAR - RMSE: {}'.format(rmse_skl_FAPAR))\n",
    "\n",
    "ax[1,1].scatter(x_tf_FAPAR, y_tf_FAPAR, c=z_tf_FAPAR)\n",
    "ax[1,1].set_title('NNet FAPAR - RMSE: {}'.format(rmse_tf_FAPAR))\n",
    "\n",
    "ax[0,2].scatter(x_skl_FCOVER, y_skl_FCOVER, c=z_skl_FCOVER)\n",
    "ax[0,2].set_title('LASSO LARS FCOVER - RMSE: {}'.format(rmse_skl_FCOVER))\n",
    "\n",
    "ax[1,2].scatter(x_tf_FCOVER, y_tf_FCOVER, c=z_tf_FCOVER)\n",
    "ax[1,2].set_title('NNet FCOVER - RMSE: {}'.format(rmse_tf_FCOVER))\n",
    "\n",
    "#plt.savefig(\"./matplotlib_outputs/uniform_replicates_w_weights_50000_samples_all_features.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all plots for the RMSE of the NN as training was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_rmse = plt.plot(numpy.sqrt(LAI_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LAI RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_rmse = plt.plot(numpy.sqrt(FAPAR_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FAPAR RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_rmse = plt.plot(numpy.sqrt(FCOVER_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FCOVER RMSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
