{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ee163-0f46-4d53-b4fa-bacbd4209666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the useful inputs are used in the Euclidean distance calculation\n",
    "# only LAI is looked at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18ae01-4c96-4dba-a89e-7e364d281ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries #\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy.matlib\n",
    "import pandas \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import os\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666da758-0a8e-4df2-a5a7-b8f840df67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the precision of the data in the Pandas Dataframes \n",
    "\n",
    "pandas.set_option(\"precision\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0781966-289a-4b9e-b3a8-ccf7ba6e9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any Tensorflow warnings \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c31e1-123c-4e7e-a678-b343889245a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MatLab data #\n",
    "\n",
    "matlabData = sio.loadmat(file_name='./data/s2_sl2p_uniform_10_replicates_sobol_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2f970-8a5b-4da9-8147-de6aed1c96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input and output data #\n",
    "\n",
    "bands = pandas.DataFrame(data=matlabData['Input']['Rho_Toc'][0][0])\n",
    "angles = pandas.DataFrame(data=matlabData['Input']['Angles'][0][0])\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e04537-7d8e-43ee-9bc6-9ab33bb56f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "cal_data = pandas.concat([bands, angles, LAI, FAPAR, FCOVER], axis=1, join='outer')\n",
    "\n",
    "cal_data.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d61adc-001e-4d0e-9735-902b29f33d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI']\n",
    "\n",
    "mean_list = [cal_data[item].mean() for item in columns]\n",
    "\n",
    "std_list = [cal_data[item].std() for item in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11450877-ce54-428b-a893-0493ee16426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_list)\n",
    "print(std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0d872-4da6-41f2-bf21-fbbb144c075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "cal_data_scaled = pandas.DataFrame(sklearn.preprocessing.StandardScaler().fit_transform(cal_data))\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fac25-0dbe-4c7e-8a85-40a3145e79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset id's #\n",
    "\n",
    "rep = 10\n",
    "\n",
    "subsets = numpy.arange(0, int(cal_data_scaled.shape[0]/10))\n",
    "\n",
    "subset_ids = numpy.matlib.repmat(subsets, 1, rep)\n",
    "\n",
    "cal_data_scaled['subset_id'] = subset_ids[0]\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67445684-69b5-49c6-8a10-42222408778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to create reference database # \n",
    "\n",
    "ref_data = cal_data_scaled.sample(n=100, ignore_index=False)\n",
    "\n",
    "ref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4be0f-e434-4924-954b-b892285c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indices to remove from the calibration database #\n",
    "\n",
    "index_list = ref_data.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce18300-eb70-41dd-ab61-54ccc4ec4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the reference data so that they start from zero\n",
    "\n",
    "ref_data = ref_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0feca-4214-4701-8ac9-0cfc5151bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the indices from calibration database that are in the reference database #\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836301d-029d-41c9-9865-31b5cfd71d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the calibration data so that they start from zero\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefc8d0-ad19-41d9-9208-72adccd547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "features_training, features_valid = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=0.3, train_size=0.7, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a9463-f8fa-4884-9997-198fca519843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "features_training = features_training.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138925fc-d31c-4385-8db6-0b4311926ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "features_valid = features_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65697f96-b91e-4ccf-be4a-9a8ce41da78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_feature_training = features_training['LAI']\n",
    "FAPAR_feature_training = features_training['FAPAR']\n",
    "FCOVER_feature_training = features_training['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6d10b-3207-48c9-8daa-a6325e295fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_feature_valid = features_valid['LAI']\n",
    "FAPAR_feature_valid = features_valid['FAPAR']\n",
    "FCOVER_feature_valid = features_valid['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef993f-aea4-4a6b-b0c2-faff7b2b6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "features_training = features_training.drop(['LAI', 'FAPAR', 'FCOVER','subset_id'], axis=1)\n",
    "features_valid = features_valid.drop(['LAI', 'FAPAR', 'FCOVER','subset_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb992d7-edf6-4687-bb25-f1771e58d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model for LAI, FAPAR, and FCOVER using LARs regression \n",
    "\n",
    "LAI_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAI_feature_model = LAI_feature_model.fit(features_training, LAI_feature_training)\n",
    "\n",
    "FAPAR_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPAR_feature_model = FAPAR_feature_model.fit(features_training, FAPAR_feature_training)\n",
    "\n",
    "FCOVER_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVER_feature_model = FCOVER_feature_model.fit(features_training, FCOVER_feature_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6ce69-3803-4c97-a9d0-8871a02cb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the LARS models\n",
    "\n",
    "LAI_feature_predicted = pandas.Series(LAI_feature_model.predict(features_valid))\n",
    "FAPAR_feature_predicted = pandas.Series(FAPAR_feature_model.predict(features_valid))\n",
    "FCOVER_feature_predicted = pandas.Series(FCOVER_feature_model.predict(features_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21506554-6ee6-48f5-ba92-b6b6404a8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the features from the LARS Model\n",
    "\n",
    "LAI_features = numpy.nonzero(LAI_feature_model.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPAR_feature_model.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVER_feature_model.coef_)[0]\n",
    "\n",
    "LAI_features = features_valid.columns[LAI_features]\n",
    "print(LAI_features)\n",
    "FAPAR_features = features_valid.columns[FAPAR_features]\n",
    "print(FAPAR_features)\n",
    "FCOVER_features = features_valid.columns[FCOVER_features]\n",
    "print(FCOVER_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb231d-fc95-4b7e-8778-188359f3b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates arrays containing the calibration and reference data \n",
    "\n",
    "ref_array = numpy.array(ref_data[LAI_features])\n",
    "\n",
    "cal_array = numpy.array(cal_data_scaled[LAI_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5f9dd-efff-46de-8dd6-3c2b4ea19434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls function from sci-kit learn for calculating the euclidean distance \n",
    "\n",
    "dist = DistanceMetric.get_metric('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0175af-4b35-4e37-975f-25bbcc0ec93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the probability for each sample in the calibration data\n",
    "\n",
    "probs = numpy.exp(-numpy.amin(dist.pairwise(cal_array,ref_array),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f498e2c-9f6a-4d59-a48f-d17e364ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled['prob'] = probs \n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47387d-52bd-4d43-9f16-32a664052e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ba6f6-e8c9-4a2e-aacc-a7579fbb7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the probabilities \n",
    "\n",
    "def normalize(data):\n",
    "    sum_prob = sum(data['prob'])\n",
    "    data['prob'] = data['prob']/sum_prob\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34571ec3-5766-4afb-8dc9-7b7e095e1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(cal_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bbefb-e504-42aa-aa10-13da187324b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace942f-d46e-4311-a8a3-8342c0204e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign max probability in each subset to every member of that subset \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled.groupby('subset_id')['prob'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14895acc-d0cc-4baa-8a8d-7b05002ad9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(cal_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf60ac9-7467-4822-9525-ba90663f0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find median probability \n",
    "\n",
    "median_prob = numpy.median(cal_data_scaled['prob'])\n",
    "\n",
    "print(median_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbe52f-c66c-4237-8b41-1eb96d1704b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight that are below the median to zero \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled['prob'].where(cal_data_scaled['prob'] > median_prob, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ec311-ed30-40e4-aee6-9ca7ecd64793",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0cb28-f668-4499-a9f4-70cf6d6255ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "training_data, valid_data = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=0.3, train_size=0.7, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cfb6c-6c7e-4d76-8322-6f1529f91268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40309ddb-1de2-49b5-85b6-2cd212cdf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5ee8c-993c-43eb-aa6a-db5151bf96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_training = training_data['LAI']\n",
    "FAPAR_training = training_data['FAPAR']\n",
    "FCOVER_training = training_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba201-30f2-486d-9ada-69f5cb405097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_valid = valid_data['LAI']\n",
    "FAPAR_valid = valid_data['FAPAR']\n",
    "FCOVER_valid = valid_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281f461-3065-483d-843d-e2d28cc909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the probabilites for training and validation \n",
    "\n",
    "training_weights = numpy.array(training_data['prob'])\n",
    "valid_weights = numpy.array(valid_data['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e9b40-8e0a-48b8-a38f-ae7fc4b4b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gee = ['cosVZA', 'cosSZA', 'cosRAA', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12']\n",
    "MatLab = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3']\n",
    "#MatLab = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "LAI_inputs = ['B3','B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12','cosVZA', 'cosSZA', 'cosRAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9d0fd-042b-4ac2-92bf-eb717d0bfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "# Explicitly subset the inputs\n",
    "training_data = training_data[MatLab]\n",
    "valid_data = valid_data[MatLab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cd0cf-042e-4c99-8604-ea963fc16d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ed162-ec16-4428-9c51-821746dfb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c0fb5-f390-42b8-8c73-c5bc2cbe2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec20cb-0821-4103-a9fa-b04112293d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history = LAI_model.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid)),\n",
    "                            callbacks=[LAI_callback]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987f14b-89e0-4aa3-bbeb-a824ebbc40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_model.save('./models/LAI_models_control')\n",
    "\n",
    "# Save LAI mean and std to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c072df-03d0-4b38-b1d5-6791314ff516",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cal_data_scaled['A1']))\n",
    "print(min(cal_data_scaled['A1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13c744-0f60-4882-bb89-2a950b619874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cal_data_scaled['A2']))\n",
    "print(min(cal_data_scaled['A2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb9460-a53d-49fd-a614-d25a34d2634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cal_data_scaled['A3']))\n",
    "print(min(cal_data_scaled['A3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e8a36-afac-4f54-8797-5870e5e48ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cal_data['A1']))\n",
    "print(min(cal_data['A1']))\n",
    "print(max(cal_data['A2']))\n",
    "print(min(cal_data['A2']))\n",
    "print(max(cal_data['A3']))\n",
    "print(min(cal_data['A3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f81ba5-fddf-4797-ad6b-9e0e28d8b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "scaling_data = [cal_data['LAI'].mean(), cal_data['LAI'].std()]\n",
    "\n",
    "with open('scale_data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(scaling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e3322-a296-4abc-b203-c08e1d919591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
