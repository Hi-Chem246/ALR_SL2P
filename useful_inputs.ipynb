{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fbc89-e982-4c54-ac04-cbb6ae3aaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the useful inputs are used in the Euclidean distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18ae01-4c96-4dba-a89e-7e364d281ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages and libraries #\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy.matlib\n",
    "import pandas \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import os\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666da758-0a8e-4df2-a5a7-b8f840df67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the precision of the data in the Pandas Dataframes \n",
    "\n",
    "pandas.set_option(\"precision\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0781966-289a-4b9e-b3a8-ccf7ba6e9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any Tensorflow warnings \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c31e1-123c-4e7e-a678-b343889245a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MatLab data #\n",
    "\n",
    "matlabData = sio.loadmat(file_name='./data/s2_sl2p_uniform_10_replicates_sobol_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2f970-8a5b-4da9-8147-de6aed1c96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input and output data #\n",
    "\n",
    "bands = pandas.DataFrame(data=matlabData['Input']['Rho_Toc'][0][0])\n",
    "angles = pandas.DataFrame(data=matlabData['Input']['Angles'][0][0])\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e04537-7d8e-43ee-9bc6-9ab33bb56f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "cal_data = pandas.concat([bands, angles, LAI, FAPAR, FCOVER], axis=1, join='outer')\n",
    "\n",
    "cal_data.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0d872-4da6-41f2-bf21-fbbb144c075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the calibration data #\n",
    "\n",
    "cal_data_scaled = pandas.DataFrame(sklearn.preprocessing.StandardScaler().fit_transform(cal_data))\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fac25-0dbe-4c7e-8a85-40a3145e79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset id's #\n",
    "\n",
    "rep = 10\n",
    "\n",
    "subsets = numpy.arange(0, int(cal_data_scaled.shape[0]/10))\n",
    "\n",
    "subset_ids = numpy.matlib.repmat(subsets, 1, rep)\n",
    "\n",
    "cal_data_scaled['subset_id'] = subset_ids[0]\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id']\n",
    "\n",
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67445684-69b5-49c6-8a10-42222408778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to create reference database # \n",
    "\n",
    "ref_data = cal_data_scaled.sample(n=100, ignore_index=False)\n",
    "\n",
    "ref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4be0f-e434-4924-954b-b892285c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indices to remove from the calibration database #\n",
    "\n",
    "index_list = ref_data.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce18300-eb70-41dd-ab61-54ccc4ec4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the reference data so that they start from zero\n",
    "\n",
    "ref_data = ref_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0feca-4214-4701-8ac9-0cfc5151bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the indices from calibration database that are in the reference database #\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.drop(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836301d-029d-41c9-9865-31b5cfd71d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the calibration data so that they start from zero\n",
    "\n",
    "cal_data_scaled = cal_data_scaled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefc8d0-ad19-41d9-9208-72adccd547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "features_training, features_valid = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=0.3, train_size=0.7, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a9463-f8fa-4884-9997-198fca519843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "features_training = features_training.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138925fc-d31c-4385-8db6-0b4311926ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "features_valid = features_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65697f96-b91e-4ccf-be4a-9a8ce41da78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_feature_training = features_training['LAI']\n",
    "FAPAR_feature_training = features_training['FAPAR']\n",
    "FCOVER_feature_training = features_training['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6d10b-3207-48c9-8daa-a6325e295fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_feature_valid = features_valid['LAI']\n",
    "FAPAR_feature_valid = features_valid['FAPAR']\n",
    "FCOVER_feature_valid = features_valid['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef993f-aea4-4a6b-b0c2-faff7b2b6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "features_training = features_training.drop(['LAI', 'FAPAR', 'FCOVER','subset_id'], axis=1)\n",
    "features_valid = features_valid.drop(['LAI', 'FAPAR', 'FCOVER','subset_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb992d7-edf6-4687-bb25-f1771e58d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model for LAI, FAPAR, and FCOVER using LARs regression \n",
    "\n",
    "LAI_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAI_feature_model = LAI_feature_model.fit(features_training, LAI_feature_training)\n",
    "\n",
    "FAPAR_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPAR_feature_model = FAPAR_feature_model.fit(features_training, FAPAR_feature_training)\n",
    "\n",
    "FCOVER_feature_model = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVER_feature_model = FCOVER_feature_model.fit(features_training, FCOVER_feature_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6ce69-3803-4c97-a9d0-8871a02cb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the LARS models\n",
    "\n",
    "LAI_feature_predicted = pandas.Series(LAI_feature_model.predict(features_valid))\n",
    "FAPAR_feature_predicted = pandas.Series(FAPAR_feature_model.predict(features_valid))\n",
    "FCOVER_feature_predicted = pandas.Series(FCOVER_feature_model.predict(features_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21506554-6ee6-48f5-ba92-b6b6404a8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the features from the LARS Model\n",
    "\n",
    "LAI_features = numpy.nonzero(LAI_feature_model.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPAR_feature_model.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVER_feature_model.coef_)[0]\n",
    "\n",
    "LAI_features = features_valid.columns[LAI_features]\n",
    "print(LAI_features)\n",
    "FAPAR_features = features_valid.columns[FAPAR_features]\n",
    "print(FAPAR_features)\n",
    "FCOVER_features = features_valid.columns[FCOVER_features]\n",
    "print(FCOVER_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb231d-fc95-4b7e-8778-188359f3b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates arrays containing the calibration and reference data \n",
    "\n",
    "ref_array = numpy.array(ref_data[LAI_features])\n",
    "\n",
    "cal_array = numpy.array(cal_data_scaled[LAI_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5f9dd-efff-46de-8dd6-3c2b4ea19434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls function from sci-kit learn for calculating the euclidean distance \n",
    "\n",
    "dist = DistanceMetric.get_metric('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0175af-4b35-4e37-975f-25bbcc0ec93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the probability for each sample in the calibration data\n",
    "\n",
    "probs = numpy.exp(-numpy.amin(dist.pairwise(cal_array,ref_array),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f498e2c-9f6a-4d59-a48f-d17e364ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled['prob'] = probs \n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47387d-52bd-4d43-9f16-32a664052e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ba6f6-e8c9-4a2e-aacc-a7579fbb7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the probabilities \n",
    "\n",
    "def normalize(data):\n",
    "    sum_prob = sum(data['prob'])\n",
    "    data['prob'] = data['prob']/sum_prob\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34571ec3-5766-4afb-8dc9-7b7e095e1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(cal_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bbefb-e504-42aa-aa10-13da187324b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability column in the calibration database #\n",
    "\n",
    "cal_data_scaled.columns = ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3', 'LAI', 'FAPAR', 'FCOVER', 'subset_id', 'prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace942f-d46e-4311-a8a3-8342c0204e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign max probability in each subset to every member of that subset \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled.groupby('subset_id')['prob'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14895acc-d0cc-4baa-8a8d-7b05002ad9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(cal_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf60ac9-7467-4822-9525-ba90663f0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find median probability \n",
    "\n",
    "median_prob = numpy.median(cal_data_scaled['prob'])\n",
    "\n",
    "print(median_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbe52f-c66c-4237-8b41-1eb96d1704b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight that are below the median to zero \n",
    "\n",
    "cal_data_scaled['prob'] = cal_data_scaled['prob'].where(cal_data_scaled['prob'] > median_prob, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ec311-ed30-40e4-aee6-9ca7ecd64793",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0cb28-f668-4499-a9f4-70cf6d6255ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates the training and validation sets from the calibration data\n",
    "\n",
    "training_data, valid_data = sklearn.model_selection.train_test_split(cal_data_scaled, test_size=0.3, train_size=0.7, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cfb6c-6c7e-4d76-8322-6f1529f91268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resets the indices in the training data so that they start from zero\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40309ddb-1de2-49b5-85b6-2cd212cdf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the indices in the validation data so that they start from zero\n",
    "\n",
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5ee8c-993c-43eb-aa6a-db5151bf96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for training\n",
    "\n",
    "LAI_training = training_data['LAI']\n",
    "FAPAR_training = training_data['FAPAR']\n",
    "FCOVER_training = training_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba201-30f2-486d-9ada-69f5cb405097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the LAI, FAPAR, and FCOVER data to be used for validation\n",
    "\n",
    "LAI_valid = valid_data['LAI']\n",
    "FAPAR_valid = valid_data['FAPAR']\n",
    "FCOVER_valid = valid_data['FCOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281f461-3065-483d-843d-e2d28cc909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the probabilites for training and validation \n",
    "\n",
    "training_weights = numpy.array(training_data['prob'])\n",
    "valid_weights = numpy.array(valid_data['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9d0fd-042b-4ac2-92bf-eb717d0bfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes that isn't needed for training and validation \n",
    "# Explicitly subset the inputs\n",
    "training_data = training_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)\n",
    "valid_data = valid_data.drop(['LAI', 'FAPAR', 'FCOVER','subset_id','prob'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ed162-ec16-4428-9c51-821746dfb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "FAPAR_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "FCOVER_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c0fb5-f390-42b8-8c73-c5bc2cbe2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec20cb-0821-4103-a9fa-b04112293d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history = LAI_model.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            sample_weight = training_weights,\n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid), valid_weights),\n",
    "                            callbacks=[LAI_callback]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987f14b-89e0-4aa3-bbeb-a824ebbc40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_model.save('./ALR_SL2P/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5a7fa-4eea-449f-a3f3-c3df3aca0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FAPAR\n",
    "\n",
    "FAPAR_history = FAPAR_model.fit(x = numpy.array(training_data), y = numpy.array(FAPAR_training),\n",
    "                                sample_weight = training_weights,\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data), numpy.array(FAPAR_valid), valid_weights),\n",
    "                                callbacks=[FAPAR_callback]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9918a53-8f26-4b04-8280-5052d64e175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FCOVER\n",
    "\n",
    "FCOVER_history = FCOVER_model.fit(x = numpy.array(training_data), y = numpy.array(FCOVER_training),\n",
    "                                  sample_weight = training_weights,\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data), numpy.array(FCOVER_valid), valid_weights),\n",
    "                                  callbacks=[FCOVER_callback]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc8bd3-09df-4dd9-966e-b842770b6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all plots for the RMSE of the NN as training was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b0a7a-f870-4efd-ba3b-a7fa1013055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_rmse = plt.plot(numpy.sqrt(LAI_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LAI RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc178364-8555-47aa-a681-f6d42fe005dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_rmse = plt.plot(numpy.sqrt(FAPAR_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FAPAR RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2517366-2bf6-4227-a13e-2a08ff12554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_rmse = plt.plot(numpy.sqrt(FCOVER_history.history['mse']))\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"FCOVER RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca6355-83d2-4199-bf64-9ab3a08c7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data\n",
    "\n",
    "LAI_predictions = pandas.Series(LAI_model.predict(numpy.array(valid_data)).flatten())\n",
    "FAPAR_predictions = pandas.Series(FAPAR_model.predict(numpy.array(valid_data)).flatten())\n",
    "FCOVER_predictions = pandas.Series(FCOVER_model.predict(numpy.array(valid_data)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701b1fc-32bb-44af-8d07-02072484225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model for LAI, FAPAR, and FCOVER using LARS regression \n",
    "\n",
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAImodel = LAImodel.fit(training_data, LAI_training)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FAPARmodel = FAPARmodel.fit(training_data, FAPAR_training)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "FCOVERmodel = FCOVERmodel.fit(training_data, FCOVER_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ef491-57db-473f-b0bc-a2c43010c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the LARS models\n",
    "\n",
    "LAI_predicted = pandas.Series(LAImodel.predict(valid_data))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(valid_data))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a804a1e-3f57-429b-85c3-b94532c3c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates the density plots \n",
    "\n",
    "def plotting_function(var, input_var, resp_var, title, xlabel, ylabel, alg, ax=None):\n",
    "    ax = ax\n",
    "    input_var = input_var*cal_data[var].std() + cal_data[var].mean()\n",
    "    resp_var = resp_var*cal_data[var].std() + cal_data[var].mean()\n",
    "    xy = numpy.vstack([input_var, resp_var])\n",
    "    z = scipy.stats.gaussian_kde(xy)(xy)\n",
    "    idx = z.argsort()\n",
    "    x = input_var[idx]\n",
    "    y = resp_var[idx]\n",
    "    z = z[idx]\n",
    "    rmse = sklearn.metrics.mean_squared_error(x, y, squared=False)\n",
    "    r_sqr = sklearn.metrics.r2_score(x, y)\n",
    "    ax.scatter(x, y, c = z)\n",
    "    plt.colorbar(mappable=ax.scatter(x, y, c = z), ax=ax)\n",
    "    ax.set_title(title + '-' + alg + ' - RMSE: {}'.format(rmse) + ' - $R^2$: {}'.format(r_sqr))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e0f7e-d569-4467-8266-f4c9b8285c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(25,15))\n",
    "\n",
    "plotting_function('LAI',LAI_valid, LAI_predicted, 'LAI','Input LAI','Predicted LAI','LARS', ax1)\n",
    "  \n",
    "plotting_function('FAPAR',FAPAR_valid, FAPAR_predicted, 'FAPAR','Input FAPAR','Predicted FAPAR','LARS', ax2)\n",
    "\n",
    "plotting_function('FCOVER',FCOVER_valid, FCOVER_predicted, 'FCOVER','Input FCOVER','Predicted FCOVER','LARS', ax3)\n",
    "\n",
    "fig.suptitle('LARS Regression', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79751b0d-91da-4151-9150-db948efb5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax4, ax5, ax6) = plt.subplots(1,3,figsize=(25,15))\n",
    "\n",
    "plotting_function('LAI', LAI_valid, LAI_predictions, 'LAI','Input LAI','Predicted LAI','NNet',ax4)\n",
    "plotting_function('FAPAR', FAPAR_valid, FAPAR_predictions, 'FAPAR','Input FAPAR','Predicted FAPAR','NNet',ax5)\n",
    "plotting_function('FCOVER', FCOVER_valid, FCOVER_predictions, 'FCOVER','Input FCOVER','Predicted FCOVER','NNet',ax6)\n",
    "\n",
    "fig.suptitle('Neural Network - with weights', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7d030-35aa-4078-a884-b256cee0b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Neural Network models for LAI, FAPAR, and FCOVER \n",
    "\n",
    "LAI_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "LAI_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FAPAR_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FAPAR_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "FCOVER_model_no_weights = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu, \n",
    "                                  input_shape=[len(training_data.keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.relu),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model_no_weights.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5b886-2a55-4042-a83c-9af3fe1122de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for LAI\n",
    "\n",
    "LAI_history_no_weights = LAI_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(LAI_training), \n",
    "                            epochs = 120,\n",
    "                            validation_data = (numpy.array(valid_data), numpy.array(LAI_valid)) \n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab42e8d-036a-4ba3-88e2-57e481fe893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FAPAR\n",
    "\n",
    "FAPAR_history_no_weights = FAPAR_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(FAPAR_training),\n",
    "                                epochs = 20, \n",
    "                                validation_data = (numpy.array(valid_data), numpy.array(FAPAR_valid))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b719ea-5854-4fa9-8da5-203aba38fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs NN Model for FCOVER\n",
    "\n",
    "FCOVER_history_no_weights = FCOVER_model_no_weights.fit(x = numpy.array(training_data), y = numpy.array(FCOVER_training),\n",
    "                                  epochs = 20, \n",
    "                                  validation_data = (numpy.array(valid_data), numpy.array(FCOVER_valid))\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf023df3-d57a-4bde-969b-997f4084513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the validation data using the \n",
    "\n",
    "LAI_predictions_no_weights = pandas.Series(LAI_model_no_weights.predict(numpy.array(valid_data)).flatten())\n",
    "FAPAR_predictions_no_weights = pandas.Series(FAPAR_model_no_weights.predict(numpy.array(valid_data)).flatten())\n",
    "FCOVER_predictions_no_weights = pandas.Series(FCOVER_model_no_weights.predict(numpy.array(valid_data)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eab879-6843-46be-a7ec-633825e8e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax7, ax8, ax9) = plt.subplots(1,3,figsize=(25,15))\n",
    "\n",
    "plotting_function('LAI', LAI_valid, LAI_predictions_no_weights, 'LAI','Input LAI','Predicted LAI','NNet',ax7)\n",
    "plotting_function('FAPAR', FAPAR_valid, FAPAR_predictions_no_weights, 'FAPAR','Input FAPAR','Predicted FAPAR','NNet',ax8)\n",
    "plotting_function('FCOVER', FCOVER_valid, FCOVER_predictions_no_weights, 'FCOVER','Input FCOVER','Predicted FCOVER','NNet',ax9)\n",
    "\n",
    "fig.suptitle('Neural Network - without weights', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c3ac6-e58f-426b-bf3f-52c2199dad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
